#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾²å§”æœƒè‡ªå‹•æ°£è±¡ç«™è³‡æ–™è™•ç†ç¨‹å¼
=============================

åŠŸèƒ½èªªæ˜ï¼š
1. ç²å–å‰ä¸€å¤©çš„è‡ªå‹•æ°£è±¡ç«™è³‡æ–™
2. æŒ‰ç¸£å¸‚åˆ†çµ„è¨ˆç®—å¹³å‡å€¼
3. å„²å­˜ç‚ºCSVæª”æ¡ˆä¸¦å¯«å…¥è³‡æ–™åº«
4. æ”¯æ´å¤šç·šç¨‹ä¸¦è¡Œè™•ç†æé«˜æ•ˆç‡
5. å…·å‚™é¢±é¢¨è­¦å ±æª¢æŸ¥åŠŸèƒ½
"""

# å¼•å…¥ç³»çµ±ç›¸é—œå‡½å¼åº«
import sys  # ç³»çµ±ç›¸é—œåŠŸèƒ½
import io   # è¼¸å…¥è¼¸å‡ºè™•ç†
import os   # ä½œæ¥­ç³»çµ±ä»‹é¢
from sqlalchemy import create_engine  # è³‡æ–™åº«é€£æ¥

# è™•ç†Windowsç³»çµ±çš„ç·¨ç¢¼å•é¡Œï¼Œè¨­å®šæ¨™æº–è¼¸å‡ºç·¨ç¢¼ç‚ºUTF-8
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")
    os.environ["PYTHONIOENCODING"] = "utf-8"

# å¼•å…¥å…¶ä»–å¿…è¦çš„å‡½å¼åº«
import requests  # HTTPè«‹æ±‚è™•ç†
import pandas as pd  # è³‡æ–™åˆ†æå’Œè™•ç†
from datetime import datetime, timedelta  # æ—¥æœŸæ™‚é–“è™•ç†
from typing import Dict, List, Any  # å‹åˆ¥æç¤º
import json  # JSONè³‡æ–™è™•ç†
from requests.adapters import HTTPAdapter  # HTTPé©é…å™¨
from urllib3.util.retry import Retry  # é‡è©¦æ©Ÿåˆ¶
from concurrent.futures import ThreadPoolExecutor, as_completed  # å¤šç·šç¨‹è™•ç†


class WeatherDataProcessor:
    """
    æ°£è±¡è³‡æ–™è™•ç†å™¨é¡åˆ¥

    è² è²¬å¾è¾²å§”æœƒAPIç²å–æ°£è±¡è³‡æ–™ï¼Œé€²è¡Œè™•ç†è¨ˆç®—å¾Œå„²å­˜åˆ°æª”æ¡ˆå’Œè³‡æ–™åº«
    å…·å‚™é¢±é¢¨è­¦å ±æª¢æŸ¥åŠŸèƒ½å’Œå¤šç·šç¨‹è™•ç†èƒ½åŠ›
    """

    def __init__(self):
        """
        åˆå§‹åŒ–æ°£è±¡è³‡æ–™è™•ç†å™¨

        è¨­å®šAPIç«¯é»ã€æª”æ¡ˆè·¯å¾‘ã€å¤šç·šç¨‹åƒæ•¸ç­‰åŸºæœ¬é…ç½®
        """
        # è¾²å§”æœƒè‡ªå‹•æ°£è±¡ç«™è³‡æ–™APIç«¯é»
        self.api_url = "https://data.moa.gov.tw/api/v1/AutoWeatherStationType/"

        # è¨­å®šè¼¸å‡ºç›®éŒ„ï¼šä½¿ç”¨ç›¸å°è·¯å¾‘æŒ‡å‘å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„data/weatherè³‡æ–™å¤¾
        self.output_dir = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "data", "weather"
        )

        # é¢±é¢¨è­¦å ±ç›¸é—œAPIç«¯é»
        self.typhoon_api_url = "https://rdc28.cwa.gov.tw/TDB/public/warning_typhoon_list/get_warning_typhoon"
        self.typhoon_main_url = (
            "https://rdc28.cwa.gov.tw/TDB/public/warning_typhoon_list/"
        )

        # å¤šç·šç¨‹è™•ç†è¨­å®š
        self.use_multithreading = True  # æ˜¯å¦å•Ÿç”¨å¤šç·šç¨‹
        self.max_workers = 4  # æœ€å¤§å·¥ä½œç·šç¨‹æ•¸

        # åŸå¸‚åç¨±åˆ°åŸå¸‚ä»£ç¢¼çš„å°æ‡‰è¡¨
        # ç”¨æ–¼å°‡APIå›å‚³çš„å®Œæ•´åŸå¸‚åç¨±è½‰æ›ç‚ºè³‡æ–™åº«ä½¿ç”¨çš„ç°¡åŒ–ä»£ç¢¼
        self.city_mapping = {
            "å—æŠ•ç¸£": "NTO",
            "è‡ºä¸­å¸‚": "TXG",
            "å°ä¸­å¸‚": "TXG",  # APIå¯èƒ½ä½¿ç”¨ç°¡é«”å­—ï¼Œä¿æŒç›¸å®¹æ€§
            "è‡ºåŒ—å¸‚": "TPE",
            "å°åŒ—å¸‚": "TPE",  # APIå¯èƒ½ä½¿ç”¨ç°¡é«”å­—ï¼Œä¿æŒç›¸å®¹æ€§
            "è‡ºå—å¸‚": "TNN",
            "å°å—å¸‚": "TNN",  # APIå¯èƒ½ä½¿ç”¨ç°¡é«”å­—ï¼Œä¿æŒç›¸å®¹æ€§
            "è‡ºæ±ç¸£": "TTT",
            "å°æ±ç¸£": "TTT",  # APIå¯èƒ½ä½¿ç”¨ç°¡é«”å­—ï¼Œä¿æŒç›¸å®¹æ€§
            "å˜‰ç¾©å¸‚": "CYI",
            "å˜‰ç¾©ç¸£": "CYQ",
            "åŸºéš†å¸‚": "KEE",
            "å®œè˜­ç¸£": "ILN",
            "å±æ±ç¸£": "PIF",
            "å½°åŒ–ç¸£": "CHA",
            "æ–°åŒ—å¸‚": "NTP",
            "æ–°ç«¹å¸‚": "HSZ",
            "æ–°ç«¹ç¸£": "HSC",
            "æ¡ƒåœ’å¸‚": "TAO",
            "æ¾æ¹–ç¸£": "PEN",
            "èŠ±è“®ç¸£": "HUA",
            "è‹—æ —ç¸£": "MIA",
            "é‡‘é–€ç¸£": "KIN",
            "é›²æ—ç¸£": "YUN",
            "é«˜é›„å¸‚": "KHH",
            "é€£æ±Ÿç¸£": "LCC",
        }

    def create_scraper_session(self):
        """
        å»ºç«‹æ¨¡æ“¬ç€è¦½å™¨çš„HTTP Session

        è¨­å®šé‡è©¦æ©Ÿåˆ¶å’Œå¸¸ç”¨çš„ç€è¦½å™¨Headersï¼Œæé«˜ç¶²ç«™çˆ¬å–æˆåŠŸç‡

        è¿”å›:
            requests.Session: é…ç½®å¥½çš„Sessionç‰©ä»¶
        """
        # å»ºç«‹æ–°çš„HTTP Session
        session = requests.Session()

        # è¨­å®šé‡è©¦ç­–ç•¥ï¼šç•¶é‡åˆ°ç‰¹å®šéŒ¯èª¤æ™‚è‡ªå‹•é‡è©¦
        retry_strategy = Retry(
            total=3,  # æœ€å¤šé‡è©¦3æ¬¡
            backoff_factor=1,  # é‡è©¦é–“éš”å€æ•¸
            status_forcelist=[429, 500, 502, 503, 504],  # éœ€è¦é‡è©¦çš„HTTPç‹€æ…‹ç¢¼
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        # å°‡é‡è©¦ç­–ç•¥å¥—ç”¨åˆ°HTTPå’ŒHTTPSé€£æ¥
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è¨­å®šå¸¸ç”¨çš„ç€è¦½å™¨Headersï¼Œæ¨¡æ“¬çœŸå¯¦çš„ç€è¦½å™¨è«‹æ±‚
        session.headers.update(
            {
                # æ¨¡æ“¬Chromeç€è¦½å™¨çš„User-Agent
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
                # æ¥å—çš„å…§å®¹é¡å‹
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                # æ¥å—çš„èªè¨€ï¼šç¹é«”ä¸­æ–‡å„ªå…ˆ
                "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
                # æ¥å—çš„ç·¨ç¢¼æ–¹å¼
                "Accept-Encoding": "gzip, deflate, br",
                # ä¿æŒé€£æ¥æ´»èº
                "Connection": "keep-alive",
                # è‡ªå‹•å‡ç´šä¸å®‰å…¨è«‹æ±‚
                "Upgrade-Insecure-Requests": "1",
            }
        )

        return session

    def check_yesterday_typhoon_warning(self, target_date=None):
        """æª¢æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦æœ‰é¢±é¢¨è­¦å ±"""

        if target_date is None:
            target_date = datetime.now() - timedelta(days=1)
        elif isinstance(target_date, str):
            target_date = datetime.strptime(target_date, "%Y-%m-%d")

        print(f"[æª¢æŸ¥] {target_date.strftime('%Y-%m-%d')} æ˜¯å¦æœ‰é¢±é¢¨è­¦å ±")

        return self.scrape_typhoon_warning_history(target_date, days_range=0)

    def scrape_typhoon_warning_history(self, target_date=None, days_range=1):
        """çˆ¬å–æŒ‡å®šæ—¥æœŸå‰Nå¤©çš„é¢±é¢¨è­¦å ±æ­·å²è³‡æ–™"""

        if target_date is None:
            target_date = datetime.now()
        elif isinstance(target_date, str):
            target_date = datetime.strptime(target_date, "%Y-%m-%d")

        query_date = target_date - timedelta(days=days_range)
        print(f"[æŸ¥è©¢] {query_date.strftime('%Y-%m-%d')} çš„é¢±é¢¨è­¦å ±è³‡æ–™")

        # å»ºç«‹session
        session = self.create_scraper_session()

        try:
            # ç¬¬ä¸€æ­¥ï¼šè¨ªå•ä¸»é é¢å»ºç«‹session
            print("[æ­¥é©Ÿ1] è¨ªå•ä¸»é é¢å»ºç«‹session...")

            response = session.get(self.typhoon_main_url, timeout=30, verify=False)
            print(f"   ä¸»é é¢ç‹€æ…‹ç¢¼: {response.status_code}")

            if response.status_code != 200:
                print("[éŒ¯èª¤] ç„¡æ³•è¨ªå•ä¸»é é¢")
                return None

            # ç¬¬äºŒæ­¥ï¼šPOSTè«‹æ±‚å–å¾—è­¦å ±è³‡æ–™
            all_warnings = []
            target_year = target_date.year

            # POSTåƒæ•¸
            post_data = {"year": str(target_year)}

            print(f"[æ­¥é©Ÿ2] å˜—è©¦POSTè«‹æ±‚...")
            print(f"   åƒæ•¸: {post_data}")

            # æ›´æ–°headers
            session.headers.update(
                {
                    "Referer": self.typhoon_main_url,
                    "X-Requested-With": "XMLHttpRequest",
                    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
                }
            )

            try:
                response = session.post(
                    self.typhoon_api_url, data=post_data, timeout=30, verify=False
                )
                print(f"   ç‹€æ…‹ç¢¼: {response.status_code}")

                if response.status_code == 200:
                    response_text = response.text.strip()

                    # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤è¨Šæ¯
                    if "No direct script access allowed" in response_text:
                        print("   [éŒ¯èª¤] é˜²è­·æ©Ÿåˆ¶é˜»æ“‹")
                        return None

                    # å˜—è©¦è§£æJSONï¼ˆè™•ç†BOMï¼‰
                    try:
                        # ç§»é™¤BOM (Byte Order Mark)
                        clean_text = response_text
                        if clean_text.startswith("\ufeff"):
                            clean_text = clean_text[1:]

                        data = json.loads(clean_text)
                        print("   [æˆåŠŸ] å–å¾—JSONè³‡æ–™ï¼")
                        print(
                            f"   [è³‡æ–™] ç­†æ•¸: {len(data) if isinstance(data, list) else '1 (dict)'}"
                        )

                        # è™•ç†è³‡æ–™
                        if isinstance(data, list) and data:
                            all_warnings.extend(data)
                            print(f"   [æˆåŠŸ] åŠ å…¥ {len(data)} ç­†è­¦å ±è³‡æ–™")

                    except json.JSONDecodeError as e:
                        print(f"   [éŒ¯èª¤] JSONè§£æéŒ¯èª¤: {e}")
                        return None

            except requests.exceptions.RequestException as e:
                print(f"   [éŒ¯èª¤] è«‹æ±‚éŒ¯èª¤: {e}")
                return None

            # è™•ç†çµæœ
            if all_warnings:
                print(f"\n[æˆåŠŸ] å–å¾— {len(all_warnings)} ç­†é¢±é¢¨è­¦å ±è³‡æ–™")

                # ç¯©é¸æŒ‡å®šæ—¥æœŸç¯„åœçš„è³‡æ–™
                filtered_warnings = self.filter_warnings_by_date(
                    all_warnings, target_date, days_range
                )

                if filtered_warnings:
                    print(f"\nğŸš¨ æ‰¾åˆ° {len(filtered_warnings)} å€‹é¢±é¢¨è­¦å ±ï¼")

                    for warning in filtered_warnings:
                        typhoon_name = warning.get("cht_name", "N/A")
                        eng_name = warning.get("eng_name", "N/A")
                        sea_start = warning.get("sea_start_datetime", "N/A")
                        sea_end = warning.get("sea_end_datetime", "N/A")

                        print(f"[è­¦å ±] é¢±é¢¨: {typhoon_name} ({eng_name})")
                        print(f"   æµ·è­¦æœŸé–“: {sea_start} ~ {sea_end}")

                    return {
                        "has_warning": True,
                        "warnings": filtered_warnings,
                        "summary": f"é¢±é¢¨è­¦å ±: {', '.join([w.get('cht_name', 'N/A') for w in filtered_warnings])}",
                    }
                else:
                    query_date = target_date - timedelta(days=days_range)
                    print(f"\n[æ­£å¸¸] {query_date.strftime('%Y-%m-%d')} æ²’æœ‰é¢±é¢¨è­¦å ±")

                    return {
                        "has_warning": False,
                        "warnings": [],
                        "summary": f"{query_date.strftime('%Y-%m-%d')} ç„¡é¢±é¢¨è­¦å ±",
                    }
            else:
                print("[éŒ¯èª¤] ç„¡æ³•å–å¾—é¢±é¢¨è­¦å ±è³‡æ–™")
                return {
                    "has_warning": False,
                    "warnings": [],
                    "summary": "ç„¡æ³•æŸ¥è©¢é¢±é¢¨è­¦å ±è³‡æ–™",
                }

        except Exception as e:
            print(f"[éŒ¯èª¤] çˆ¬èŸ²éŒ¯èª¤: {e}")

        finally:
            session.close()

        return None

    def filter_warnings_by_date(self, warnings, target_date, days_range):
        """ç¯©é¸æŒ‡å®šæ—¥æœŸç¯„åœå…§çš„è­¦å ±"""
        if days_range == 0:
            query_date = target_date
        else:
            query_date = target_date - timedelta(days=days_range)

        filtered = []
        for warning in warnings:
            # æª¢æŸ¥è­¦å ±æ˜¯å¦åœ¨æŸ¥è©¢æ—¥æœŸç•¶å¤©æœ‰æ•ˆ
            sea_start = warning.get("sea_start_datetime")
            sea_end = warning.get("sea_end_datetime")

            if sea_start:
                try:
                    start_dt = datetime.strptime(sea_start[:19], "%Y-%m-%d %H:%M:%S")

                    # è™•ç†çµæŸæ™‚é–“ç‚ºç©ºçš„æƒ…æ³ï¼ˆé€²è¡Œä¸­çš„é¢±é¢¨è­¦å ±ï¼‰
                    if sea_end and sea_end.strip():
                        end_dt = datetime.strptime(sea_end[:19], "%Y-%m-%d %H:%M:%S")
                    else:
                        # å¦‚æœçµæŸæ™‚é–“ç‚ºç©ºï¼Œå‡è¨­è­¦å ±ä»åœ¨é€²è¡Œä¸­ï¼Œè¨­å®šä¸€å€‹æœªä¾†æ™‚é–“
                        end_dt = datetime.now() + timedelta(days=30)  # å‡è¨­æœ€å¤š30å¤©

                    # æª¢æŸ¥æŸ¥è©¢æ—¥æœŸæ˜¯å¦åœ¨è­¦å ±æœŸé–“å…§
                    query_start = query_date.replace(hour=0, minute=0, second=0)
                    query_end = query_date.replace(hour=23, minute=59, second=59)

                    # å¦‚æœè­¦å ±æœŸé–“èˆ‡æŸ¥è©¢æ—¥æœŸæœ‰é‡ç–Šï¼Œå°±ç®—ç¬¦åˆ
                    if not (end_dt < query_start or start_dt > query_end):
                        filtered.append(warning)

                except Exception as e:
                    continue

        return filtered

    def get_weather_data(
        self, start_date: str, end_date: str = None
    ) -> List[Dict[str, Any]]:
        """
        å¾è¾²å§”æœƒAPIç²å–æŒ‡å®šæ—¥æœŸç¯„åœçš„æ°£è±¡è³‡æ–™

        åƒæ•¸:
            start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)
            end_date: çµæŸæ—¥æœŸ (å¯é¸ï¼Œé è¨­èˆ‡é–‹å§‹æ—¥æœŸç›¸åŒ)

        è¿”å›:
            List[Dict]: æ°£è±¡ç«™è³‡æ–™åˆ—è¡¨
        """
        # å¦‚æœæ²’æœ‰æŒ‡å®šçµæŸæ—¥æœŸï¼Œå‰‡èˆ‡é–‹å§‹æ—¥æœŸç›¸åŒ
        if end_date is None:
            end_date = start_date

        print(f"[ç²å–] æ­£åœ¨ç²å– {start_date} åˆ° {end_date} çš„æ°£è±¡è³‡æ–™...")

        # è¨­å®šæŸ¥è©¢åƒæ•¸
        params = {"Start_time": start_date, "End_time": end_date}

        try:
            # ç™¼é€HTTP GETè«‹æ±‚åˆ°è¾²å§”æœƒAPI
            response = requests.get(
                self.api_url, params=params, timeout=60, verify=False
            )
            # æª¢æŸ¥HTTPç‹€æ…‹ç¢¼ï¼Œå¦‚æœéŒ¯èª¤å‰‡æ‹‹å‡ºä¾‹å¤–
            response.raise_for_status()

            # è§£æJSONå›æ‡‰
            data = response.json()
            if "Data" in data:
                weather_data = data["Data"]
                print(f"[æˆåŠŸ] ç²å– {len(weather_data)} ç­†æ°£è±¡è³‡æ–™")
                return weather_data
            else:
                print("[éŒ¯èª¤] APIå›æ‡‰ä¸­æ²’æœ‰Dataæ¬„ä½")
                return []

        except requests.exceptions.RequestException as e:
            print(f"[éŒ¯èª¤] APIè«‹æ±‚å¤±æ•—: {e}")
            return []
        except json.JSONDecodeError as e:
            print(f"[éŒ¯èª¤] JSONè§£æå¤±æ•—: {e}")
            return []

    def get_yesterday_weather_data(self) -> List[Dict[str, Any]]:
        """ç²å–å‰ä¸€å¤©çš„æ°£è±¡è³‡æ–™ï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰"""
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        return self.get_weather_data(yesterday)

    def filter_valid_stations(
        self, weather_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """éæ¿¾å‡ºæœ‰æ•ˆçš„æ°£è±¡ç«™è³‡æ–™ï¼ˆæœ‰å®Œæ•´æ•¸æ“šçš„ç«™é»ï¼‰ä¸¦æ’é™¤æŒ‡å®šåŸå¸‚"""
        valid_data = []
        # å®šç¾©è¦æ’é™¤çš„åŸå¸‚åç¨±
        excluded_city_names = {
            "å˜‰ç¾©å¸‚",
            "åŸºéš†å¸‚",
            "æ–°ç«¹å¸‚",
            "æ¾æ¹–ç¸£",
            "é‡‘é–€ç¸£",
            "é€£æ±Ÿç¸£",
        }

        for record in weather_data:
            # æª¢æŸ¥æ˜¯å¦æœ‰åŸå¸‚è³‡è¨Šå’Œå¿…è¦çš„æ°£è±¡æ•¸æ“š
            city = record.get("CITY", "")
            temp = record.get("TEMP")
            pres = record.get("PRES")

            # éæ¿¾æ¢ä»¶ï¼šæœ‰åŸå¸‚è³‡è¨Šä¸”æœ‰åŸºæœ¬æ°£è±¡æ•¸æ“šï¼Œä¸”ä¸åœ¨æ’é™¤æ¸…å–®ä¸­
            if (
                city
                and temp is not None
                and pres is not None
                and city not in excluded_city_names
            ):
                valid_data.append(record)

        print(
            f"[éæ¿¾] æœ‰æ•ˆæ°£è±¡ç«™è³‡æ–™: {len(valid_data)} ç­†ï¼ˆå·²æ’é™¤ {len(excluded_city_names)} å€‹æŒ‡å®šåŸå¸‚ï¼‰"
        )
        return valid_data

    def calculate_city_averages(
        self, weather_data: List[Dict[str, Any]], target_date: str = None
    ) -> pd.DataFrame:
        """è¨ˆç®—å„ç¸£å¸‚çš„æ°£è±¡è³‡æ–™å¹³å‡å€¼"""
        print("[è¨ˆç®—] æ­£åœ¨è¨ˆç®—å„ç¸£å¸‚å¹³å‡å€¼...")

        # è½‰æ›ç‚ºDataFrame
        df = pd.DataFrame(weather_data)

        # è½‰æ›æ•¸å€¼æ¬„ä½
        numeric_fields = ["TEMP", "HUMD", "PRES", "WDSD", "H_24R"]
        for field in numeric_fields:
            df[field] = pd.to_numeric(df[field], errors="coerce")

        # æŒ‰åŸå¸‚åˆ†çµ„è¨ˆç®—å¹³å‡å€¼
        city_averages = (
            df.groupby("CITY")
            .agg(
                {
                    "PRES": "mean",  # StnPres
                    "TEMP": "mean",  # Temperature
                    "HUMD": "mean",  # RH
                    "WDSD": "mean",  # WS
                    "H_24R": "mean",  # Precp
                }
            )
            .round(2)
        )

        # é‡æ–°å‘½åæ¬„ä½
        city_averages.columns = ["StnPres", "Temperature", "RH", "WS", "Precp"]

        # é‡è¨­ç´¢å¼•ï¼Œè®“CITYæˆç‚ºä¸€èˆ¬æ¬„ä½
        city_averages.reset_index(inplace=True)

        # å°‡åŸå¸‚åç¨±è½‰æ›ç‚ºåŸå¸‚ä»£è™Ÿ
        city_averages["city_id"] = city_averages["CITY"].map(self.city_mapping)

        # ç§»é™¤æ²’æœ‰å°æ‡‰ä»£è™Ÿçš„åŸå¸‚
        city_averages = city_averages.dropna(subset=["city_id"])

        # åŠ å…¥è§€æ¸¬æ™‚é–“
        if target_date is None:
            target_date = (datetime.now() - timedelta(days=1)).strftime("%Y/%m/%d")
        else:
            # å°‡ YYYY-MM-DD æ ¼å¼è½‰æ›ç‚º YYYY/MM/DD æ ¼å¼
            target_date = target_date.replace("-", "/")
        city_averages["ObsTime"] = target_date

        # æª¢æŸ¥é¢±é¢¨ç‹€æ³ï¼ˆä½¿ç”¨æŒ‡å®šæ—¥æœŸï¼‰
        if target_date:
            check_date = target_date.replace("/", "-")  # è½‰æ›å› YYYY-MM-DD æ ¼å¼
            typhoon_result = self.check_yesterday_typhoon_warning(check_date)
        else:
            typhoon_result = self.check_yesterday_typhoon_warning()

        if typhoon_result and typhoon_result["has_warning"]:
            city_averages["typhoon"] = "1"
            # å–ç¬¬ä¸€å€‹é¢±é¢¨çš„è‹±æ–‡åç¨±
            first_typhoon = typhoon_result["warnings"][0]
            city_averages["typhoon_name"] = first_typhoon.get("eng_name", "")
        else:
            city_averages["typhoon"] = "0"
            city_averages["typhoon_name"] = ""

        # é‡æ–°æ’åˆ—æ¬„ä½é †åº
        city_averages = city_averages[
            [
                "city_id",
                "ObsTime",
                "StnPres",
                "Temperature",
                "RH",
                "WS",
                "Precp",
                "typhoon",
                "typhoon_name",
            ]
        ]

        print(f"[å®Œæˆ] {len(city_averages)} å€‹ç¸£å¸‚çš„å¹³å‡å€¼è¨ˆç®—")

        # é¡¯ç¤ºè™•ç†çš„åŸå¸‚
        print("è™•ç†çš„åŸå¸‚:")
        for _, row in city_averages.iterrows():
            print(
                f"  {row['city_id']}: æº«åº¦ {row['Temperature']:.1f}Â°C, æ¿•åº¦ {row['RH']:.1f}%"
            )

        return city_averages

    def save_to_csv(self, data: pd.DataFrame, target_date: str = None) -> str:
        """å„²å­˜è³‡æ–™ç‚ºCSVæª”æ¡ˆ"""
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)

        # ç”¢ç”Ÿæª”æ¡ˆåç¨±
        if target_date is None:
            date_str = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        else:
            # å°‡ YYYY-MM-DD æ ¼å¼è½‰æ›ç‚º YYYYMMDD æ ¼å¼
            date_str = target_date.replace("-", "")
        filename = f"daily_weather_{date_str}.csv"
        filepath = os.path.join(self.output_dir, filename)

        try:
            data.to_csv(filepath, index=False, encoding="utf-8-sig")
            print(f"[å„²å­˜] è³‡æ–™å·²å„²å­˜è‡³: {filepath}")
            return filepath
        except Exception as e:
            print(f"[éŒ¯èª¤] å„²å­˜æª”æ¡ˆå¤±æ•—: {e}")
            raise

    def process_single_date(self, date_str: str) -> str:
        """è™•ç†å–®ä¸€æ—¥æœŸçš„æ°£è±¡è³‡æ–™"""
        print(f"\n[è™•ç†] {date_str}")
        
        try:
            # 1. ç²å–æŒ‡å®šæ—¥æœŸçš„æ°£è±¡è³‡æ–™
            weather_data = self.get_weather_data(date_str)
            if not weather_data:
                print(f"[è­¦å‘Š] ç„¡æ³•ç²å– {date_str} çš„æ°£è±¡è³‡æ–™ï¼Œè·³éæ­¤æ—¥æœŸ")
                return None

            # 2. éæ¿¾æœ‰æ•ˆçš„æ°£è±¡ç«™è³‡æ–™
            valid_data = self.filter_valid_stations(weather_data)
            if not valid_data:
                print(f"[è­¦å‘Š] {date_str} æ²’æœ‰æœ‰æ•ˆçš„æ°£è±¡ç«™è³‡æ–™ï¼Œè·³éæ­¤æ—¥æœŸ")
                return None

            # 3. è¨ˆç®—å„ç¸£å¸‚å¹³å‡å€¼
            city_averages = self.calculate_city_averages(valid_data, date_str)
            if city_averages.empty:
                print(f"[è­¦å‘Š] {date_str} ç„¡æ³•è¨ˆç®—ç¸£å¸‚å¹³å‡å€¼ï¼Œè·³éæ­¤æ—¥æœŸ")
                return None

            # 4. é¡¯ç¤ºçµæœæ‘˜è¦
            print(f"\n[æ‘˜è¦] {date_str} è™•ç†çµæœ:")
            print(f"   è™•ç†ç¸£å¸‚æ•¸: {len(city_averages)}")
            print(f"   å¹³å‡æº«åº¦ç¯„åœ: {city_averages['Temperature'].min():.1f}Â°C - {city_averages['Temperature'].max():.1f}Â°C")
            print(f"   å¹³å‡æ¿•åº¦ç¯„åœ: {city_averages['RH'].min():.1f}% - {city_averages['RH'].max():.1f}%")

            # 5. å„²å­˜ç‚ºCSV
            filepath = self.save_to_csv(city_averages, date_str)

            # 6. å¯«å…¥è³‡æ–™åº«ï¼ˆé¿å…é‡è¤‡ï¼šç›¸åŒ ObsTime + city_id ä¸é‡è¤‡å¯«å…¥ï¼‰
            try:
                # æª¢æŸ¥æ˜¯å¦æœ‰ psycopg2
                try:
                    import psycopg2

                    print(f"[è³‡æ–™åº«] æ­£åœ¨é€£æ¥ PostgreSQL ({date_str})...")

                    engine = create_engine(
                        "postgresql+psycopg2://lorraine:0000@111.184.52.4:9527/postgres"
                    )
                    table_name = "weather_data"

                    # è®€å–å·²å­˜åœ¨éµå€¼ï¼ˆåŒä¸€å¤©å·²å­˜åœ¨çš„ city_idï¼‰
                    obs_time = date_str.replace("-", "/")
                    try:
                        existing = pd.read_sql(
                            f'SELECT city_id FROM {table_name} WHERE "ObsTime" = %(obs)s',
                            con=engine,
                            params={"obs": obs_time},
                        )
                        existing_ids = set(existing["city_id"].astype(str))
                        print(f"[è³‡æ–™åº«] ç™¼ç¾å·²å­˜åœ¨ {len(existing_ids)} ç­†ç›¸åŒæ—¥æœŸè³‡æ–™ ({date_str})")
                    except Exception as db_error:
                        print(f"[è³‡æ–™åº«] æŸ¥è©¢ç¾æœ‰è³‡æ–™å¤±æ•—ï¼š{db_error}")
                        existing_ids = set()

                    # æº–å‚™è¦å¯«å…¥çš„è³‡æ–™ï¼ˆæ’é™¤å·²å­˜åœ¨éµå€¼ï¼‰
                    df_to_insert = city_averages.copy()
                    df_to_insert["city_id"] = df_to_insert["city_id"].astype(str)
                    if existing_ids:
                        df_to_insert = df_to_insert[~df_to_insert["city_id"].isin(existing_ids)]

                    # å°‡ NaN è½‰ç‚º Noneï¼Œä»¥ä¾¿å¯«å…¥ç‚º NULL
                    df_to_insert = df_to_insert.where(pd.notna(df_to_insert), None)

                    if len(df_to_insert) > 0:
                        df_to_insert.to_sql(table_name, engine, if_exists="append", index=False)
                        print(f"[è³‡æ–™åº«] å¯«å…¥æˆåŠŸ ({date_str})ï¼šæ–°å¢ {len(df_to_insert)} ç­†ï¼Œç•¥é {len(city_averages) - len(df_to_insert)} ç­†ï¼ˆå·²å­˜åœ¨ï¼‰")
                    else:
                        print(f"[è³‡æ–™åº«] å¯«å…¥ç•¥é ({date_str})ï¼šç›¸åŒæ—¥æœŸèˆ‡ city_id çš„è³‡æ–™å·²å­˜åœ¨")

                    engine.dispose()  # é—œé–‰é€£æ¥

                except ImportError:
                    print(f"[è­¦å‘Š] ç„¡æ³•åŒ¯å…¥ psycopg2ï¼Œè·³éè³‡æ–™åº«å¯«å…¥ ({date_str})")

            except Exception as e:
                print(f"[è³‡æ–™åº«] å¯«å…¥å¤±æ•— ({date_str})ï¼š{e}")

            return filepath

        except Exception as e:
            print(f"[éŒ¯èª¤] è™•ç† {date_str} ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def process_weather_data(self, start_date: str = None, end_date: str = None):
        """ä¸»è¦è™•ç†æµç¨‹"""
        if start_date is None:
            # å¦‚æœæ²’æŒ‡å®šæ—¥æœŸï¼Œé è¨­ç‚ºæ˜¨å¤©
            start_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

        if end_date is None:
            end_date = start_date

        print(f"[é–‹å§‹] è™•ç†æ°£è±¡è³‡æ–™ ({start_date} åˆ° {end_date})...")
        print("=" * 50)

        # ç”Ÿæˆæ—¥æœŸç¯„åœ
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # ç”Ÿæˆæ‰€æœ‰æ—¥æœŸåˆ—è¡¨
        date_list = []
        current_dt = start_dt
        while current_dt <= end_dt:
            date_list.append(current_dt.strftime("%Y-%m-%d"))
            current_dt += timedelta(days=1)

        print(f"[è³‡è¨Š] å…±éœ€è™•ç† {len(date_list)} å€‹æ—¥æœŸ")
        
        # æ±ºå®šæ˜¯å¦ä½¿ç”¨å¤šç·šç¨‹
        if self.use_multithreading and len(date_list) > 1:
            print(f"[å¤šç·šç¨‹] ä½¿ç”¨ {self.max_workers} ç·šç¨‹ä¸¦è¡Œè™•ç†")
            processed_files = []
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤ä»»å‹™
                future_to_date = {executor.submit(self.process_single_date, date_str): date_str 
                                for date_str in date_list}
                
                # ç­‰å¾…å®Œæˆ
                for future in as_completed(future_to_date):
                    date_str = future_to_date[future]
                    try:
                        result = future.result()
                        if result:
                            processed_files.append(result)
                    except Exception as e:
                        print(f"[éŒ¯èª¤] å¤šç·šç¨‹è™•ç† {date_str} å¤±æ•—: {e}")
        else:
            print("[å–®ç·šç¨‹] é †åºè™•ç†")
            processed_files = []
            
            for date_str in date_list:
                result = self.process_single_date(date_str)
                if result:
                    processed_files.append(result)

        print(f"\n[å®Œæˆ] æ°£è±¡è³‡æ–™è™•ç†å®Œæˆï¼")
        print(f"æˆåŠŸè™•ç† {len(processed_files)} å€‹æª”æ¡ˆ")
        print("=" * 50)

        return processed_files


def main():
    """ä¸»ç¨‹å¼"""
    # ============== åŸ·è¡Œåƒæ•¸è¨­å®šå€ ==============
    USE_MULTITHREADING = True          # æ˜¯å¦ä½¿ç”¨å¤šç·šç¨‹ (True/False)
    MAX_WORKERS = 10                   # æœ€å¤§ç·šç¨‹æ•¸ (å»ºè­°è¨­ç‚ºä½ çš„CPUç·šç¨‹æ•¸-2)
    
    # ============== æ—¥æœŸè¨­å®šå€ ==============
    # ä¿®æ”¹é€™è£¡çš„æ—¥æœŸä¾†æŒ‡å®šè¦ä¸‹è¼‰çš„ç¯„åœ
    # START_DATE = "2022-01-01"  # é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
    # END_DATE = "2025-08-13"  # çµæŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œè¨­ç‚º None å‰‡åªä¸‹è¼‰é–‹å§‹æ—¥æœŸç•¶å¤©

    # å¦‚æœè¦ä¸‹è¼‰æ˜¨å¤©çš„è³‡æ–™ï¼Œè¨­å®šç‚º None
    START_DATE = None
    END_DATE = None
    # =======================================

    processor = WeatherDataProcessor()
    processor.use_multithreading = USE_MULTITHREADING
    processor.max_workers = MAX_WORKERS

    if START_DATE is None:
        # é è¨­æ¨¡å¼ï¼šä¸‹è¼‰æ˜¨å¤©çš„è³‡æ–™
        processor.process_weather_data()
    else:
        # æŒ‡å®šæ—¥æœŸæ¨¡å¼
        processor.process_weather_data(START_DATE, END_DATE)


if __name__ == "__main__":
    main()
